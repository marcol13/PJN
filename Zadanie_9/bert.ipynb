{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D48rgmQSB0B6"
   },
   "source": [
    "# Laboratoria 9: BERT i atencja\n",
    "\n",
    "\n",
    "### Zadanie 1 (3 pkt), atencja dekodera względem (en)kodera\n",
    "\n",
    "Poniżej znajdują się dwie macierze, `encoder_states` oraz `decoder_states` reprezentujące stan warstwy ukrytej po przetworzeniu każdego slowa z enkodera i dekodera. Pojedynczy stan warstwy ukrytej zawiera embedding o dlugosci = 3. W enkoderze mamy 4 stany warstwy ukrytej RNNów, gdyż przetwarzamy sekwencję 4 tokenów.\n",
    "\n",
    "W dekoderze mamy 5 tokenów, które powinny być wygenerowane z sekwencji przetwarzanej (en)koderem.\n",
    "\n",
    "Zadanie polega na:\n",
    "a) Obliczniu podobieństwa wszystkich embeddingów z dekodera (queries) względem wszystkich embeddingów kolejnych stanów (en)kodera (keys) [pamiętajcie, że macierze potrafią w transponowanie. W `NumPy` macierz transponujemy za pomocą `macierz.T`]\n",
    "\n",
    "b) Na utworzonej macierzy podobieństwa należy wykonać softmax (zaimportowany z scipy). Uwaga:  pamiętajcie, żeby aplikować softmax w dobrym wymiarze. Wszystkie stany ukryte enkodera powinny zostac zasoftmaksowane względem zadanego stanu dekodera, nie odwrotnie. W scipy, funkcja softmax zawiera argument axis, który może pomóc.\n",
    "\n",
    "c) Należy wykorzystać macierz atencji z kroku b) i `encoder_states` do wygenerowania macierzy zawierającej wektory kontekstu dla każdego tokenu z dekodera.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "vQsum9iYATge"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  4.806   2.376   7.762   1.129]\n",
      " [ 12.164 -12.645  73.935   3.636]\n",
      " [ 27.79  -16.962  94.002   7.137]\n",
      " [ 18.702  -5.184  42.616   4.501]\n",
      " [ 64.38   49.86   56.21   14.45 ]]\n",
      "[[4.91780633e-02 4.32948093e-03 9.45248312e-01 1.24414389e-03]\n",
      " [1.49003187e-27 2.50486173e-38 1.00000000e+00 2.94803216e-31]\n",
      " [1.75587568e-29 6.44090821e-49 1.00000000e+00 1.88369172e-38]\n",
      " [4.11416552e-11 1.74069934e-21 1.00000000e+00 2.79811669e-17]\n",
      " [9.99716568e-01 4.94220792e-07 2.82937800e-04 2.06801368e-22]]\n",
      "[[ 9.69108631  0.35799187  0.59163688]\n",
      " [10.2         0.2         0.3       ]\n",
      " [10.2         0.2         0.3       ]\n",
      " [10.2         0.2         0.3       ]\n",
      " [ 1.20254471  3.39909302  5.59850122]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "\n",
    "# scipy.special.softmax(x, axis=None)\n",
    "\n",
    "encoder_states = np.array(\n",
    "    [[1.2, 3.4, 5.6],    # embedding z warstwy ukrytej enkodera w kroku 1,  np. dla slowa Ala\n",
    "    [-2.3, 0.2, 7.2],   # embedding z warstwy ukrytej enkodera w kroku 2,  np. dla slowa ma\n",
    "    [10.2, 0.2, 0.3],   # embedding z warstwy ukrytej enkodera w kroku 3,  np. dla slowa kota\n",
    "    [0.4, 0.7, 1.2]]    # embedding z warstwy ukrytej enkodera w kroku 4,  np. dla tokenu <EOS> (koniec sekwencji)\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "decoder_states = np.array(\n",
    "    [[0.74, 0.23, 0.56],  # embedding z warstwy ukrytej dekodera w kroku 1,  np. przed wygenerowaniem slowa Alice\n",
    "    [7.23, 0.12, 0.55],  # embedding z warstwy ukrytej dekodera w kroku 2,  np. przed wygenerowaniem slowa owns\n",
    "    [9.12, 4.23, 0.44],  # embedding z warstwy ukrytej dekodera w kroku 3,  np. przed wygenerowaniem slowa a\n",
    "    [4.1, 3.23, 0.5],    # embedding z warstwy ukrytej dekodera w kroku 4,  np. przed wygenerowaniem slowa cat\n",
    "    [5.2, 3.1, 8.5]]     # embedding z warstwy ukrytej dekodera w kroku 5,  np. przed wygenerowaniem slowa cat\n",
    ")\n",
    "\n",
    "#a)\n",
    "\n",
    "similarity_states = [[np.dot(dec, enc) for enc in encoder_states] for dec in decoder_states]\n",
    "\n",
    "similarity_states = np.stack(similarity_states)\n",
    "\n",
    "print(similarity_states)\n",
    "\n",
    "#b)\n",
    "\n",
    "softmax_states = softmax(similarity_states, 1)\n",
    "\n",
    "print(softmax_states)\n",
    "\n",
    "#c)\n",
    "\n",
    "result = [[np.dot(soft, enc) for enc in encoder_states.T] for soft in softmax_states]\n",
    "\n",
    "result = np.stack(result)\n",
    "\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lwhDwdaTnPku"
   },
   "source": [
    "**Oczekiwane wartości:**\n",
    "\n",
    "a) \n",
    "[[  4.806   2.376   7.762   1.129]\n",
    " [ 12.164 -12.645  73.935   3.636]\n",
    " [ 27.79  -16.962  94.002   7.137]\n",
    " [ 18.702  -5.184  42.616   4.501]\n",
    " [ 64.38   49.86   56.21   14.45 ]] \n",
    "\n",
    "\n",
    "b) \n",
    "[[4.91780633e-02 4.32948093e-03 9.45248312e-01 1.24414389e-03]\n",
    " [1.49003187e-27 2.50486173e-38 1.00000000e+00 2.94803216e-31]\n",
    " [1.75587568e-29 6.44090821e-49 1.00000000e+00 1.88369172e-38]\n",
    " [4.11416552e-11 1.74069934e-21 1.00000000e+00 2.79811669e-17]\n",
    " [9.99716568e-01 4.94220792e-07 2.82937800e-04 2.06801368e-22]] \n",
    "\n",
    "c) \n",
    "[[ 9.69108631  0.35799187  0.59163688]\n",
    " [10.2         0.2         0.3       ]\n",
    " [10.2         0.2         0.3       ]\n",
    " [10.2         0.2         0.3       ]\n",
    " [ 1.20254471  3.39909302  5.59850122]]\n",
    " \n",
    " (albo to samo transponowane)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a9220arNHo1V"
   },
   "source": [
    "## Zadanie 2 (2 punkty): tokenizacja tekstu \n",
    "\n",
    "Korzystając z biblioteki transformers (https://huggingface.co/transformers/) wczytaj tokenizator BERTa (BERT to już wytrenowany (pretrenowany) model, oparty o ideę transformera (a w zasadzie o jego enkoder)). Ponieważ model jest gotowy i można go wykorzystać do generowania embeddingów tokenów, ważnym jest, aby tokenizacja była przeprowadzona identycznie do tego jak podczas trenowania BERTa.\n",
    "\n",
    "Wybierzmy pretrenowany tokenizator o nazwie `bert-base-uncased` i zobaczmy jaki będzie efekt tokenizacji na tekście zawartym w zmiennej `text_to_tokenize`.\n",
    "\n",
    "Zwróć uwagę na to, że niektóre rzadkie słowa zostały podzielone na subtokeny -- zgodnie z algorytmem WordPiece jaki omawialiśmy na przedostatnim spotkaniu.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "yxcU6_pYNgfP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.19.2-py3-none-any.whl (4.2 MB)\n",
      "     ---------------------------------------- 4.2/4.2 MB 1.5 MB/s eta 0:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 22.0.4; however, version 22.1 is available.\n",
      "You should consider upgrading via the 'c:\\users\\marcin\\appdata\\local\\programs\\python\\python38\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyyaml>=5.1\n",
      "  Downloading PyYAML-6.0-cp38-cp38-win_amd64.whl (155 kB)\n",
      "     -------------------------------------- 155.4/155.4 KB 1.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\marcin\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from transformers) (20.9)\n",
      "Collecting huggingface-hub<1.0,>=0.1.0\n",
      "  Downloading huggingface_hub-0.6.0-py3-none-any.whl (84 kB)\n",
      "     ---------------------------------------- 84.4/84.4 KB 1.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: requests in c:\\users\\marcin\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from transformers) (2.25.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\marcin\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from transformers) (2022.3.2)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\marcin\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from transformers) (1.22.3)\n",
      "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
      "  Downloading tokenizers-0.12.1-cp38-cp38-win_amd64.whl (3.3 MB)\n",
      "     ---------------------------------------- 3.3/3.3 MB 1.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\marcin\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from transformers) (4.63.0)\n",
      "Collecting filelock\n",
      "  Downloading filelock-3.7.0-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\marcin\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.0.1)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\marcin\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from packaging>=20.0->transformers) (2.4.7)\n",
      "Requirement already satisfied: colorama in c:\\users\\marcin\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\marcin\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from requests->transformers) (1.26.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\marcin\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\marcin\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from requests->transformers) (2020.12.5)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\marcin\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from requests->transformers) (4.0.0)\n",
      "Installing collected packages: tokenizers, pyyaml, filelock, huggingface-hub, transformers\n",
      "Successfully installed filelock-3.7.0 huggingface-hub-0.6.0 pyyaml-6.0 tokenizers-0.12.1 transformers-4.19.2\n"
     ]
    }
   ],
   "source": [
    "# Uruchom mnie proszę\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "uMLn-hE8L6Zh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 1045, 1005, 2310, 4149, 1037, 2047, 14246, 2226, 2197, 2095, 2009, 2001, 16216, 14821, 19387, 2595, 24622, 2692, 102]\n",
      "['[CLS]', 'i', \"'\", 've', 'bought', 'a', 'new', 'gp', '##u', 'last', 'year', 'it', 'was', 'ge', '##force', 'rt', '##x', '306', '##0', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "text_to_tokenize = \"I've bought a new GPU last year it was GeForce RTX 3060\"\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "tokens = tokenizer.encode(text_to_tokenize)\n",
    "\n",
    "print(tokens)\n",
    "print(tokenizer.convert_ids_to_tokens(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cOr1m9n6Mw3V"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RFwEeX2GOxEp"
   },
   "source": [
    "## Zadanie 3 (brak punktów):\n",
    "Poniżej znajduje się kod wykorzystujący przygotowane wcześniej zmienne `tokenizer` i `tokens` i które dla każdego tokenu z tokens generuje embedding. W odróżnieniu od GloVe, te embeddingi są świadome kontekstu w jakim właśnie występują. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "XHlN4iH8P0Sv"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70f2babe29134e3a8cc1828c260c3fe9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/420M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([22, 768])\n",
      "tensor([-2.5072e-01,  6.7097e-01, -4.9258e-01, -5.1823e-01, -4.1960e-01,\n",
      "        -4.2894e-01,  7.2098e-01, -2.9315e-01,  1.1021e-01, -2.0099e-01,\n",
      "         6.0429e-02,  3.3400e-02,  3.1436e-02,  6.4501e-02,  9.9129e-01,\n",
      "        -1.7249e-01,  8.2772e-01, -5.9561e-01,  2.3217e-01, -1.3435e-01,\n",
      "        -5.5493e-01, -2.5491e-01, -2.3414e-02,  6.2318e-01,  2.9955e-02,\n",
      "        -5.6500e-01,  3.6730e-01, -8.7795e-01, -2.9278e-02, -2.5605e-01,\n",
      "         1.7754e+00, -1.1409e+00, -5.3760e-01,  5.6952e-01, -3.9512e-01,\n",
      "        -1.1072e+00,  5.5010e-01, -2.5960e-01,  1.5085e-01,  1.6989e-01,\n",
      "        -9.3601e-01, -6.4144e-01,  1.8195e-01,  4.0841e-01, -7.2718e-01,\n",
      "        -1.3888e+00,  2.3231e-01,  3.8964e-01,  5.4087e-01,  4.9496e-01,\n",
      "         4.6612e-03,  2.8136e-01,  1.6244e-02,  4.7729e-01, -1.1086e-01,\n",
      "         6.9736e-01, -2.0195e-01, -7.7565e-01, -6.0757e-01, -8.0674e-01,\n",
      "         1.2961e+00, -4.9921e-02, -4.7822e-01, -2.7278e-01,  3.4258e-01,\n",
      "         3.2852e-01, -2.8506e-01,  6.3931e-01, -1.5600e-01,  2.2565e-01,\n",
      "        -4.8359e-01,  1.3212e+00, -7.5749e-01, -4.8886e-01, -4.2954e-02,\n",
      "        -2.2001e-01, -3.3964e-01,  7.4898e-01,  4.4901e-01, -9.1197e-01,\n",
      "        -2.9953e-01,  7.2120e-01, -1.0444e+00,  9.5173e-01, -4.3537e-01,\n",
      "        -1.0809e-01,  1.6845e+00, -6.1293e-01, -2.2784e-01,  7.8584e-02,\n",
      "        -7.4083e-02, -6.3195e-01,  2.7118e-01, -2.3148e-01,  6.1839e-01,\n",
      "         8.9477e-01, -5.2970e-01,  7.5033e-01,  1.4189e-04, -5.6829e-03,\n",
      "        -1.0088e-01,  1.7208e-01,  1.7762e-01,  2.4752e-01, -9.7200e-01,\n",
      "        -4.5464e-01,  2.4893e-01,  2.6617e-01, -6.1268e-01,  9.4321e-01,\n",
      "        -4.3098e-01, -5.7640e-01, -1.3064e+00, -9.0630e-01, -4.3822e-02,\n",
      "         5.3578e-01, -1.0038e-01,  4.7138e-01,  4.1614e-01,  3.1561e-01,\n",
      "         4.2914e-01,  1.3614e-01,  3.4287e-01,  7.7498e-01,  3.4926e-02,\n",
      "        -2.0249e-01, -2.8227e-01,  3.2808e-01, -4.9863e-01,  7.7245e-02,\n",
      "        -2.7877e-01,  6.6745e-01, -9.0161e-01,  2.5075e-01, -2.0076e-01,\n",
      "        -6.5625e-01,  5.7189e-01,  2.5140e-01, -1.0615e+00, -4.9208e-01,\n",
      "        -4.0206e-01, -1.4251e-01,  2.7987e-01, -5.2541e-01, -5.0659e-01,\n",
      "        -1.9390e-01, -3.0769e-01, -7.9919e-01, -5.4309e-01, -8.2936e-02,\n",
      "         7.4859e-02, -4.7390e-01,  1.5651e+00, -3.2748e-01,  4.6811e-02,\n",
      "        -5.4717e-01, -5.9103e-01, -2.6665e-01, -1.0148e+00, -7.8753e-01,\n",
      "         6.3463e-01,  1.8726e-01,  3.4709e-02,  2.2398e-01,  2.9647e-01,\n",
      "         7.3697e-01,  3.6264e-01,  2.6689e-01, -9.3656e-01, -2.1241e-01,\n",
      "         2.9862e-01, -5.8394e-01, -1.6521e-01, -1.1438e-01, -1.9204e-01,\n",
      "         7.9415e-01,  7.3574e-01, -2.3224e-01, -2.7288e-01,  4.6900e-01,\n",
      "        -5.7861e-01,  7.9337e-01, -3.8035e-01, -3.6261e-01,  9.9948e-01,\n",
      "         4.9878e-01, -1.3206e-01,  2.4553e-01,  8.3620e-01,  3.5730e-01,\n",
      "        -7.5290e-01,  1.4355e-01, -1.2158e-01,  2.0597e-01,  7.0747e-01,\n",
      "        -2.5979e-01,  3.1073e-01,  2.8540e-01, -4.9018e-01,  1.4783e-01,\n",
      "        -3.4676e-01, -1.7229e-01, -3.2262e-01,  2.3094e-01, -1.1260e+00,\n",
      "        -7.7775e-01,  5.2716e-01,  2.5536e-01,  3.9809e-01, -8.6096e-01,\n",
      "         1.2811e-01,  5.6213e-01,  8.6642e-02,  2.6144e-01,  8.8950e-01,\n",
      "         1.6288e-01, -3.7304e-01, -6.5537e-01, -1.5932e-01, -4.0495e-02,\n",
      "        -3.8064e-01, -8.0465e-01, -7.3599e-01,  7.1481e-01,  3.9496e-01,\n",
      "         1.9168e-01,  6.1785e-01, -1.2732e+00,  1.4739e-01,  1.7131e-01,\n",
      "        -4.3454e-03, -1.1844e+00,  9.7411e-01,  7.1728e-01, -5.3550e-01,\n",
      "        -3.5657e-01, -8.2406e-02, -7.0026e-02,  2.7606e-01, -4.9430e-01,\n",
      "         6.5143e-01,  1.2284e+00, -1.4725e-01, -8.5263e-01,  2.9336e-01,\n",
      "         5.2065e-01, -2.7800e-01, -4.8735e-01, -7.8970e-01, -6.0507e-01,\n",
      "        -2.2713e-01,  1.6103e-01,  7.6517e-02, -6.4038e-02, -3.8067e-01,\n",
      "        -1.1637e+00,  4.3910e-01,  3.7715e-01, -5.4895e-01, -2.2775e-01,\n",
      "         2.9189e-01,  2.0333e-01,  1.4449e-01,  3.8051e-01,  9.6175e-02,\n",
      "         2.9128e-01, -4.7460e-01,  1.0787e+00,  5.8274e-01,  6.8903e-01,\n",
      "        -1.7706e-01,  5.9125e-01, -7.4776e-01,  4.8527e-01, -4.3962e-01,\n",
      "         4.0941e-01,  2.5865e-01, -4.2640e-01,  3.2906e-01,  3.2082e-01,\n",
      "         4.3603e-01,  2.8832e-01,  4.3936e-01, -2.0669e-01,  7.2508e-02,\n",
      "         3.7453e-01, -3.1969e-01, -1.8038e-01,  1.9806e-01, -3.4510e-01,\n",
      "         2.5657e-01,  4.3779e-01, -9.7010e-01,  8.5625e-02,  2.2452e-01,\n",
      "         1.3830e-01, -1.1931e-01,  8.6323e-01,  4.2565e-01,  4.8340e-01,\n",
      "        -1.3715e-01, -3.6435e-01,  6.6884e-01, -2.1879e-01,  2.6008e-01,\n",
      "        -5.4081e-01, -1.0584e-01,  4.3800e-01, -3.5285e+00,  1.0393e+00,\n",
      "         6.8556e-01, -4.5574e-02,  1.0255e-01, -5.6009e-01,  1.5610e-01,\n",
      "         6.1995e-03, -4.8036e-01,  2.7610e-01, -7.4639e-01,  6.8746e-01,\n",
      "        -3.2241e-02, -9.6864e-02,  1.4006e-01,  1.0095e+00,  6.2048e-02,\n",
      "         1.4190e-02, -6.7828e-01,  1.6120e-01,  3.8854e-02, -2.2671e-01,\n",
      "        -6.2658e-01,  1.7415e-01,  6.4518e-01, -5.5405e-02, -4.3306e-01,\n",
      "        -4.6573e-01, -2.6518e-01, -3.4744e-01,  2.3010e-01, -1.9478e-01,\n",
      "         2.7601e-01,  7.3729e-01,  4.5696e-01, -2.7811e-01, -6.0661e-01,\n",
      "        -4.4061e-01,  2.0748e-01, -8.2100e-01, -5.7810e-01, -1.7982e+00,\n",
      "        -2.6778e-01, -6.9237e-01,  4.7657e-01, -4.5106e-01,  1.3999e-01,\n",
      "        -3.5338e-02,  1.2202e-01,  2.7973e-01,  9.7734e-01, -4.3794e-01,\n",
      "        -1.3057e-01,  1.7011e-01,  1.9391e-01, -9.2233e-02,  9.1141e-01,\n",
      "         8.4983e-01, -8.2326e-01, -1.1872e+00, -1.1412e-01,  1.0054e-01,\n",
      "        -2.4022e+00,  6.2652e-04,  3.9889e-01, -1.3027e+00, -6.0441e-01,\n",
      "        -6.5938e-01, -4.4333e-01,  2.7138e-01, -5.7977e-01,  1.0619e+00,\n",
      "         3.3047e-01, -1.3075e-01, -1.8833e-02,  6.5419e-01, -1.2197e-02,\n",
      "        -8.1177e-03, -5.8081e-01, -7.5556e-01, -1.1085e-01, -2.2931e-01,\n",
      "         2.9868e-01, -1.6613e-02, -7.2057e-01,  7.0196e-01, -2.8000e-01,\n",
      "        -3.0249e-01, -6.0865e-01, -4.8216e-01,  1.1499e+00,  2.6901e-01,\n",
      "         1.5399e-01, -4.3076e-01,  6.0808e-01,  4.8028e-01,  6.1236e-01,\n",
      "        -5.7832e-01, -9.9820e-02,  3.7455e-02,  6.9241e-01,  6.6685e-01,\n",
      "        -1.7096e-01,  1.3341e-01, -2.6259e-02, -9.2935e-02, -8.9088e-01,\n",
      "         3.9692e-01,  1.0077e+00,  2.4019e-01, -6.2887e-02, -4.5323e-01,\n",
      "         6.3530e-02, -2.4354e-01, -1.0218e-01, -1.6996e-01,  5.5201e-01,\n",
      "         4.2442e-01,  9.9400e-01, -8.6077e-01, -3.4683e-01,  8.7599e-01,\n",
      "        -1.3201e+00, -4.4552e-01, -9.4293e-01,  1.1320e+00, -4.9743e-01,\n",
      "         1.3157e-01,  1.0914e-01,  4.7622e-01,  2.1188e-01, -3.6313e-01,\n",
      "        -5.1438e-01,  5.0133e-01, -1.4575e-01,  3.0441e-01,  5.0796e-01,\n",
      "        -5.1420e-01, -4.2130e-01, -4.9774e-01, -1.8622e-01,  5.2932e-02,\n",
      "         3.9473e-01, -1.8589e-01, -2.0329e-01,  2.5174e-01, -5.6399e-01,\n",
      "         6.7910e-02, -2.1104e-01,  1.4177e-01, -5.9070e-01, -7.0039e-01,\n",
      "        -2.6060e-01,  4.6711e-03,  9.5406e-02, -9.5633e-02, -4.8776e-01,\n",
      "        -4.3567e-01,  3.8951e-01, -1.2953e+00,  1.2504e+00, -5.8558e-01,\n",
      "         1.4909e-01,  6.3606e-01, -5.2395e-01,  1.4021e+00,  9.9293e-02,\n",
      "         7.1972e-01,  2.0119e-01, -3.3472e-01,  6.3004e-01, -5.8863e-01,\n",
      "        -1.5449e-01, -1.2373e+00, -8.7538e-02,  1.3198e+00,  7.8494e-02,\n",
      "         4.1547e-01, -6.9252e-01, -2.5660e-01,  5.4729e-01,  4.4862e-01,\n",
      "         7.6278e-01, -8.3585e-02,  2.1258e-01, -3.2740e-01,  2.0820e-01,\n",
      "        -9.3260e-01,  2.0672e-02,  7.1265e-01,  4.3508e-01,  3.9370e-01,\n",
      "         2.6058e-01, -8.9311e-02, -3.5719e-01, -7.4850e-01, -9.3828e-02,\n",
      "        -7.3373e-02,  3.6900e-02,  7.2832e-02,  3.8246e-01, -2.5848e-01,\n",
      "         2.2264e-01, -2.8088e-01, -2.4798e-01,  3.0396e-01, -3.8130e-01,\n",
      "        -4.1006e-02,  3.7752e-01,  8.8494e-03, -2.6007e-02, -2.3824e-01,\n",
      "        -4.8407e-01, -4.1840e-01, -6.1814e-01, -2.5800e-01, -2.6836e-02,\n",
      "         4.7077e-01, -6.0046e-01,  2.4455e-02,  6.8970e-01,  2.3053e-01,\n",
      "        -3.2936e-01, -5.2798e-01,  2.7338e-01, -2.6304e-01, -8.2081e-01,\n",
      "        -1.1439e-01, -3.3547e-01, -1.0305e+00, -3.4745e-01, -1.2257e+00,\n",
      "         3.9114e-02,  3.2384e-01, -7.1237e-01, -6.9731e-02,  1.7060e-02,\n",
      "         1.2092e-01,  1.1851e+00,  2.5303e-01,  4.1018e-01,  8.0098e-01,\n",
      "        -3.8807e-01,  7.6226e-01,  9.1654e-01,  3.9960e-01, -1.1709e+00,\n",
      "        -2.1743e-01,  5.6086e-01,  3.4817e-03, -9.2760e-02, -9.4850e-02,\n",
      "         9.4250e-02, -2.2511e-01, -1.4183e-01, -5.5716e-02, -6.2266e-02,\n",
      "         8.0847e-01, -7.6285e-01,  1.5325e-01,  1.7899e-01,  6.4278e-01,\n",
      "        -9.0756e-02,  3.3071e-02,  3.9859e-01, -1.0290e-01,  6.6818e-02,\n",
      "         8.1719e-01, -9.6309e-01, -4.2657e-02, -1.1790e+00, -2.1971e-01,\n",
      "        -8.9282e-02,  4.2265e-01, -4.9179e-02, -9.1705e-02, -3.3576e-01,\n",
      "         1.3477e-01, -4.3502e-03,  6.5144e-02, -3.3650e-02, -3.1465e-01,\n",
      "         5.5630e-01,  1.2457e+00,  7.4268e-01, -1.7762e-01,  3.8517e-01,\n",
      "         7.5662e-01,  4.1500e-01, -3.6377e-01,  2.7181e-01,  3.8858e-02,\n",
      "        -3.1206e-01, -6.1914e-01, -3.0703e-02,  1.8124e-01,  5.8598e-01,\n",
      "         2.8569e-01, -7.7424e-01,  1.0020e-01,  4.0022e-01, -8.5991e-01,\n",
      "         3.0288e-01,  1.2563e+00,  4.8584e-02, -2.7957e-01,  4.5496e-01,\n",
      "        -9.3569e-01, -7.3572e-01, -4.6949e-01, -2.3953e-01,  4.8084e-01,\n",
      "         5.8785e-01, -7.5730e-02, -3.9905e-02, -8.2899e-01, -7.6406e-01,\n",
      "         6.8582e-01,  7.1622e-01, -1.8139e-02,  4.3290e-02, -6.3783e-01,\n",
      "         7.9102e-01, -6.5861e-03,  5.5544e-01, -7.3049e-01,  3.9959e-01,\n",
      "        -3.5539e-01, -2.1295e-01,  1.1242e+00, -2.8820e-01,  2.7288e-01,\n",
      "         9.9585e-01,  1.0674e-01,  1.4223e-01, -1.6254e-01, -1.3170e+00,\n",
      "         2.3404e-01,  7.2104e-01,  3.8302e-02, -2.6254e-01,  8.1433e-01,\n",
      "         1.0831e+00, -9.2653e-02,  1.0792e-01,  2.6781e-01,  6.7019e-01,\n",
      "        -1.0602e+00,  2.2264e-01, -3.3384e-01,  6.5826e-01,  2.6947e-01,\n",
      "        -4.2092e-01,  6.0116e-02, -5.2973e-02,  7.4785e-01,  8.1796e-02,\n",
      "         1.0344e+00,  5.2222e-01,  5.3765e-01, -4.4621e-01,  3.3116e-01,\n",
      "         8.9198e-02, -2.2375e-01, -1.1548e-01, -7.9489e-02, -2.0257e-01,\n",
      "        -1.3367e-01, -6.6945e-01,  1.7329e-01, -1.5400e-01, -5.1237e-02,\n",
      "        -2.7734e-01, -1.1227e+00,  5.6599e-01,  7.4277e-01, -6.1809e-01,\n",
      "        -2.6360e-01,  3.5203e-01, -2.9705e-01, -5.1477e-01, -1.4604e-01,\n",
      "         6.2330e-01, -5.9417e-01, -8.1516e-01,  9.4605e-03, -7.8874e-01,\n",
      "        -2.5653e-01, -3.8816e-01, -1.2129e+00,  7.5813e-01,  1.6157e-01,\n",
      "         7.5447e-01,  1.6695e-01, -6.1655e-01, -2.3562e-01,  5.7512e-01,\n",
      "        -7.0147e-02,  7.6117e-01,  5.1227e-01, -1.9243e-01, -1.2417e-01,\n",
      "        -1.2462e+00,  2.4939e-01,  1.4211e-01,  9.4800e-02,  8.1658e-01,\n",
      "         7.1034e-01, -5.6315e-01, -2.7331e-01,  5.4040e-01, -4.4885e-01,\n",
      "         1.0858e-01,  8.0877e-01,  9.8364e-03,  7.8691e-02,  8.8247e-01,\n",
      "         3.2441e-01, -1.1686e+00, -1.6398e-01, -3.6475e-01, -9.3243e-01,\n",
      "         1.1171e-01,  2.0775e-01, -1.0241e-01, -3.0792e-01,  1.7031e-01,\n",
      "        -2.9702e-01,  2.4021e-01,  1.3733e-01, -5.1209e-01, -3.7198e-02,\n",
      "        -3.3480e-01,  3.5795e-01, -1.7044e-01, -4.0163e-01,  3.2840e-01,\n",
      "         1.0237e+00,  2.4576e-01,  6.1517e-01,  5.2563e-01,  9.3822e-01,\n",
      "        -1.6664e-01,  7.0635e-01,  1.1216e+00,  4.6979e-01,  1.4519e-01,\n",
      "         1.3233e-01, -9.1781e-01, -7.6829e-01, -5.0700e-02, -6.7280e-01,\n",
      "         5.0238e-01, -3.9764e-01, -2.1177e-01, -4.7022e-01, -8.4852e-01,\n",
      "         3.7605e-02,  6.3738e-01,  3.0032e-01])\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertModel\n",
    "import torch\n",
    "\n",
    "model = BertModel.from_pretrained('bert-base-uncased', return_dict=True)  \n",
    "model.eval()  # nie chcemy trenowac modelu, tylko go wykorzystac\n",
    "\n",
    "tokens_with_specials = ['CLS'] + tokens + ['SEP']  # BERT wymaga specjalnych tokenów [CLS] na poczatku i [SEP] separaującego pary zdań (BERT jest trenowany parami zdań)\n",
    "tokens_with_specials = tokenizer.convert_tokens_to_ids(tokens_with_specials)  # zamiana listy tokenow na listę identyfikatorów (liczb) ze slownika\n",
    "tokens_tensor = torch.tensor([tokens_with_specials])  # zamiana na tensor, opakowanie w batch\n",
    "\n",
    "segments = torch.tensor([[1] * len(tokens_with_specials)])  # wygeneruj maskę mówiącą o tym które tokeny nalezą do zdania 1, a ktore do 2. W naszym zadaniu wszystkie tokeny naleza do zdania 1\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(tokens_tensor, segments)  # wygenerujmy embeddingi BERTem\n",
    "    tokens_embeddings = outputs['last_hidden_state'][0]  # wez pierwszy batch danych i ostatnią warstwę\n",
    "    print(tokens_embeddings.shape)  # 20x768, mamy 20 (sub)tokenów, (18 wlasciwych + cls + sep) i kazdy mapowany jest na wektor liczb o dlugosci 768\n",
    "    print(tokens_embeddings[1])  # wez embedding pierwszego subtokenu z sekwencji (przeskakujemy CLS token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BWi18mj6T_S5"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Zadania10-2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
